# -*- coding: utf-8 -*-
"""Copy of PySpark Course.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16biGuLBXzY8FbaiP89dcf70vHY6RNj-v

https://github.com/LinkedInLearning/apache-pyspark-by-example-802868
https://www.linkedin.com/learning/apache-pyspark-by-example/working-with-joins?autoSkip=true&resume=false&u=36492188
"""

'''
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q htttp://www-us.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz
!tar xf spark-2.3.1-bin-hadoop2.7.tgz
!pip install -q findspark
'''

# innstall java
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# install spark (change the version number if needed)
!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz

# unzip the spark file to the current folder
!tar xf spark-3.0.0-bin-hadoop3.2.tgz

# set your spark folder to your system path environment.
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.0-bin-hadoop3.2"


# install findspark using pip
!pip install -q findspark

!ls

import findspark
findspark.init()
from pyspark import SparkContext
sc = SparkContext.getOrCreate()
sc

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
spark

!wget https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD
!ls -l

# Renaming the file
!mv rows.csv\?accessType\=DOWNLOAD reported-crimes.csv
!ls -l

!ls

from pyspark.sql.functions import to_timestamp, col, lit
rc = spark.read.csv('reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))
rc.show(5)

!ls

type(rc)

rc.dtypes

rc.printSchema()

len(rc.columns)

rc.columns

from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegralType, DoubleType, TimestampType, IntegerType

schema1 = StructType([StructField('ID', StringType(), True),
            StructField('Case Number', StringType(), True),
            StructField('Date', TimestampType(), True),
            StructField('Block', StringType(), True),
            StructField('IUCR', StringType(), True),
            StructField('Primary Type', StringType(), True),
            StructField('Description', StringType(), True),
            StructField('Location Description', StringType(), True),
            StructField('Arrest', StringType(), True),
            StructField('Domestic', BooleanType(), True),
            StructField('Beat', StringType(), True),
            StructField('District', StringType(), True),
            StructField('Ward', StringType(), True),
            StructField('Community Area', StringType(), True),
            StructField('FBI Code', StringType(), True),
            StructField('X Coordinate', StringType(), True),
            StructField('Y Coordinate', StringType(), True),
            StructField('Year', IntegerType(), True),
            StructField('Updated On', StringType(), True),
            StructField('Latitude', DoubleType(), True),
            StructField('Longitude', DoubleType(), True),
            StructField('Location', StringType(), True),


            ])

labels = rc.columns
datatype = [StringType(), StringType(), TimestampType(),StringType(),StringType(),StringType(),StringType(),StringType(),
            StringType(), BooleanType(),StringType(),StringType(),StringType(),StringType(),StringType(),StringType(),
            StringType(), IntegerType(), StringType(), DoubleType(), DoubleType(), StringType()]
col_dt_joined = zip(labels, datatype)
schema = StructType([StructField(x[0], x[1], True) for x in col_dt_joined])
schema

rc1 = spark.read.csv('reported-crimes.csv', schema=schema)
rc1.show()

rc1.select(col('ID')).show(5)

rc1.select('ID', 'Case Number').show(3)

rc11 = rc1.withColumn('newID', col('ID')+'A')

rc11.columns

rc11.select(col('newID'))

rc11 = rc11.withColumnRenamed('newID', 'NewID')

rc11.columns

rc11.select(rc11.NewID).show(3)

# adding column with name one and alln entries=1
rc11.withColumn('One', lit(1))

rc11.show()

rc11.columns

rc11 = rc11.drop('One')

rc11.show(3)

rc11.filter(col('ID')>1)

rc11.select('Arrest').distinct().show(3)

!ls

# rc11.orderBY('ID') # note that the orderBy has been depreciated from the version 3
rc11.sort(col('ID')).show(5)

# dataframe unions
# rc = spark.read.csv('reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))
# let's get the crimes in one day November 2018-11-12
one_day = spark.read.csv('reported-crimes.csv', header=True).withColumn('Date', to_timestamp(col('Date'), 'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') == '2018-11-12')
one_day.show(5)

rc.union(one_day).sort(col('Date'), ascending=False).show(5)

rc.select(col('Primary Type')).distinct().show()

ls

# what are the top 10 number of reported crimes by primary type in descending order
rc.groupBy('Primary Type').count().sort('count', ascending=False).show(5)

# Find top 3 locations for reported crimes
rc.groupBy('Location Description').count().sort('count', ascending=False).show(3)

percentage_of_arrest = 100 * rc.filter(col('Arrest')=='true').count() / rc.count()

percentage_of_arrest

"""**Built-In function in PySpark**"""

from pyspark.sql.functions import mean
rc.select(mean('X Coordinate')).show()

from pyspark.sql import functions
print(dir(functions))

"""**String Functions**"""

# convert primary type column to lower, upper, and then choose the first 4 characters
from pyspark.sql.functions import lower, upper, substring
rc.select(lower(col('Primary Type')), upper(col('Primary Type')), substring(col('Primary Type'), 1, 4)).show()

help(substring)

"""**Numeric Functions**

choose the oldest and most recent dates
"""

from pyspark.sql.functions import min, max
rc.select(max(col('Date')), min(col('Date'))).show()

"""What is three days earlier than the oldest day and 3 days later than the most recent day"""

from pyspark.sql.functions import date_add, date_sub
help(date_add)

rc.select(date_sub(min(col('Date')), 3), date_add(max(col('Date')),3)).show()

"""**Working with dates and timestamps in Pyspark**"""

from pyspark.sql.functions import to_date, to_timestamp, lit

help(to_date)

df = spark.createDataFrame([('2019-12-25 13:30:00', )], ['Christmas'])
df.show(1)

df.select(to_date(col('Christmas'), 'yyyy-MM-dd HH:mm:ss').alias('Date'), to_timestamp(col('Christmas'), 'yyyy-MM-dd HH:mm:ss').alias('TimeStamp')).show(1)

df.select('Christmas').collect()

df1 = spark.createDataFrame([('25/Dec/2019 13:30:00', )], ['Christmas'])
df1.show(1)

df1.select(to_date(col('Christmas'), 'dd/MMM/yyyy HH:mm:ss').alias('Date'), to_timestamp(col('Christmas'), 'dd/MMM/yyyy HH:mm:ss').alias('TimeStamp')).show(1)

df2 = spark.createDataFrame([('12/25/2019 01:30:00 PM', )], ['Christmas'])
df2.show(1, truncate=False)

df2.select(to_date(col('Christmas'), 'MM/dd/yyyy hh:mm:ss a').alias('Date'), to_timestamp(col('Christmas'), 'MM/dd/yyyy hh:mm:ss a').alias('TimeStamp')).show(1)

ls

nrc = spark.read.csv('reported-crimes.csv', header=True)
nrc.show(5,truncate=False)

nrc.withColumn('Date', to_timestamp(col('Date'), 'MM/dd/yyyy hh:mm:ss a')).show(5)

nrc.withColumn('Date', to_date(col('Date'), 'MM/dd/yyyy hh:mm:ss a')).show(5)

"""**User-Defined Function**"""



"""**Joins**"""

!wget O police-station.csv https://data.cityofchicago.org/resource/z8bn-74gv.csv

ls

# Renaming the file
!mv z8bn-74gv.csv police-station.csv
!ls -l

ps = spark.read.csv('police-station.csv', header=True)
ps.show(5)

ps.select(col('district')).distinct().show()

nrc.columns

nrc.select(col('District')).distinct().show()

from pyspark.sql.functions import lpad

ps.select(lpad(col('district'), 3, '0')).show()

# create a new column in ps dataframe
ps = ps.withColumn('district_formatted', lpad(col('district'), 3, '0'))

ps.columns

ps.select(ps.district_formatted).show(5)

ls

nrc.join(ps, nrc.District == ps.district_formatted, how='leftouter').drop(
 'address',
 'city',
 'state',
 'zip',
 'website',
 'phone',
 'fax',
 'tty',
 'x_coordinate',
 'y_coordinate',
 'latitude',
 'longitude',
 'location').show()

nrc.cache()

# Challenge: what is the most frequently reported noncriminal activities
nrc.select('Primary Type').distinct().show(35, truncate=False)

nrc.select('Primary Type', 'Description').filter((col('Primary Type')=='NON - CRIMINAL') | (col('Primary Type')=='NON-CRIMINAL') | (col('Primary Type')=='NON-CRIMINAL (SUBJECT SPECIFIED)')).groupBy('Description').count().sort('count', ascending=False).show()

# Which day of the week has the most numbers of reported crimes
# extracting day of the week

nrc.select(col('Date')).show(truncate=False)

nrc.withColumn('Date', to_date(col('Date'), 'MM/dd/yyyy hh:mm:ss a')).show(5)

from pyspark.sql.functions import dayofweek
nrc = nrc.withColumn('day_of_week', dayofweek(to_date(col('Date'), 'MM/dd/yyyy hh:mm:ss a')))

nrc.groupBy('day_of_week').count().sort('count', ascending=False).show()

